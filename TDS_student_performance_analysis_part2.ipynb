{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDS Project: Part 2 - Advanced Model Analysis, Optimization, and Conclusions\n",
    "**Group Members:**\n",
    "\n",
    "- Adir Elmakais - 316413640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Guide\n",
    "\n",
    "#### Use Python 3.12.0\n",
    "\n",
    "To get started with the project, ensure you are using **Python 3.12.0**.\n",
    "\n",
    "1. **Install Python 3.12.0**:\n",
    "   - Download the installer for Python 3.12.0 from the [official Python website](https://www.python.org/downloads/release/python-3120/).\n",
    "   - During the installation, make sure to check the box **\"Add Python to PATH\"**.\n",
    "\n",
    "2. **macOS: Install `libomp`**:\n",
    "   - For macOS users, you need to install `libomp` for compatibility with XGBoost. Run the following command:\n",
    "     ```bash\n",
    "     brew install libomp\n",
    "     ```\n",
    "\n",
    "3. **Install Required Packages**:\n",
    "   - Once Python 3.12.0 is installed, you can install the necessary packages listed in the `requirements.txt` file by running the following command in your terminal:\n",
    "     ```bash\n",
    "     pip install -r requirements.txt\n",
    "     ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In the second part of the TDS Project, we aim to enhance our initial machine learning pipeline by addressing the weaknesses identified during error analysis. This involves optimizing the model through hyperparameter tuning, feature engineering, and other advanced techniques to improve its predictive performance. Subsequently, we'll analyze the improved model, compare it with the baseline, and draw meaningful conclusions from our findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Baseline Model and Data\n",
    "We'll begin by loading the cleaned dataset and the baseline model saved in Part 1. This will allow us to build upon the existing pipeline and apply further optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset loaded successfully.\n",
      "Baseline model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Setting plot styles\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Paths to the cleaned data and the baseline model\n",
    "cleaned_data_path = os.path.join('data', 'StudentPerformaceFactorsClean.csv')\n",
    "model_path = os.path.join('models', 'pipeline_model.joblib')\n",
    "\n",
    "# Loading the cleaned dataset\n",
    "data_cleaned = pd.read_csv(cleaned_data_path)\n",
    "print(\"Cleaned dataset loaded successfully.\")\n",
    "\n",
    "# Loading the baseline model\n",
    "pipeline = joblib.load(model_path)\n",
    "print(\"Baseline model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Error Analysis Conclusions & Work Plan**\n",
    "   \n",
    "   - **Residual Distribution**\n",
    "     - **Observation**: Slight heteroscedasticity observed in residuals, with increased variance for higher predicted scores.\n",
    "     - **Action**: Investigate transformations of the target variable or features to stabilize variance.\n",
    "   \n",
    "   - **Subgroup Performance**\n",
    "     - **Observation**: Lower performance for students with Low and High parental involvement levels.\n",
    "     - **Action**: Explore interactions between parental involvement and other features or incorporate additional relevant features.\n",
    "   \n",
    "   - **Feature Importance**\n",
    "     - **Observation**: Attendance, Hours_Studied, and Previous_Scores are significant predictors, but other features may also play crucial roles.\n",
    "     - **Action**: Conduct feature engineering to create new features or transform existing ones to capture more information.\n",
    "   \n",
    "   - **Bias in Predictions**\n",
    "     - **Observation**: Slight tendency towards underestimation in predictions.\n",
    "     - **Action**: Adjust the model to reduce bias, possibly by addressing data imbalance or refining the loss function.\n",
    "   \n",
    "   - **Outliers**\n",
    "     - **Observation**: Presence of outliers affecting model performance.\n",
    "     - **Action**: Implement robust scaling or outlier detection methods to mitigate their impact.\n",
    "   \n",
    "2. **Work Plan**\n",
    "   \n",
    "   - **Hyperparameter Tuning**: Optimize XGBoost parameters to enhance model performance.\n",
    "   - **Feature Engineering**: Create new features and transform existing ones based on domain knowledge and EDA insights.\n",
    "   - **Handling Outliers**: Apply techniques to detect and handle outliers effectively.\n",
    "   - **Data Balancing**: If applicable, ensure that the model is not biased towards certain subgroups by balancing the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Improving Model Performance\n",
    "\n",
    "### a. Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning involves adjusting the model’s parameters to find the optimal configuration that minimizes error and maximizes performance. We’ll use GridSearchCV to perform an exhaustive search over specified parameter values for the XGBoost regressor.\n",
    "\n",
    "Step-by-Step Implementation:\n",
    "\t1.\tDefine Features and Target\n",
    "\t2.\tIdentify Categorical and Numerical Features\n",
    "\t3.\tHandle Categorical Data\n",
    "\t4.\tPerform Train-Test Split\n",
    "\t5.\tSet Up and Run GridSearchCV\n",
    "\t6.\tSave the Improved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__subsample': [0.7, 0.8, 0.9],\n",
    "    'model__colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Define features and target\n",
    "X = data_cleaned.drop('Exam_Score', axis=1)\n",
    "y = data_cleaned['Exam_Score']\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Convert boolean columns to integers if any\n",
    "for col in categorical_features:\n",
    "    if X[col].dtype == 'bool':\n",
    "        X[col] = X[col].astype(int)\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Hyperparameter tuning completed.\")\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Best R2 Score\n",
    "print(f\"Best R² Score from GridSearchCV: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Update the pipeline with best parameters\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Save the improved model\n",
    "improved_model_path = os.path.join('models', 'improved_pipeline_model.joblib')\n",
    "joblib.dump(best_pipeline, improved_model_path)\n",
    "print(f\"Improved model saved at '{improved_model_path}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
