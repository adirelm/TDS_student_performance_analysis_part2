{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDS Project: Part 2 - Advanced Model Analysis, Optimization, and Conclusions\n",
    "**Group Members:**\n",
    "\n",
    "- Adir Elmakais - 316413640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Guide\n",
    "\n",
    "#### Use Python 3.12.0\n",
    "\n",
    "To get started with the project, ensure you are using **Python 3.12.0**.\n",
    "\n",
    "1. **Install Python 3.12.0**:\n",
    "   - Download the installer for Python 3.12.0 from the [official Python website](https://www.python.org/downloads/release/python-3120/).\n",
    "   - During the installation, make sure to check the box **\"Add Python to PATH\"**.\n",
    "\n",
    "2. **macOS: Install `libomp`**:\n",
    "   - For macOS users, you need to install `libomp` for compatibility with XGBoost. Run the following command:\n",
    "     ```bash\n",
    "     brew install libomp\n",
    "     ```\n",
    "\n",
    "3. **Install Required Packages**:\n",
    "   - Once Python 3.12.0 is installed, you can install the necessary packages listed in the `requirements.txt` file by running the following command in your terminal:\n",
    "     ```bash\n",
    "     pip install -r requirements.txt\n",
    "     ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In the second part of the TDS Project, we aim to enhance our initial machine learning pipeline by addressing the weaknesses identified during error analysis. This involves optimizing the model through hyperparameter tuning, feature engineering, and other advanced techniques to improve its predictive performance. Subsequently, we'll analyze the improved model, compare it with the baseline, and draw meaningful conclusions from our findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Baseline Model and Data\n",
    "We'll begin by loading the cleaned dataset and the baseline model saved in Part 1. This will allow us to build upon the existing pipeline and apply further optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset loaded successfully.\n",
      "Baseline model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Setting plot styles\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Paths to the cleaned data and the baseline model\n",
    "cleaned_data_path = os.path.join('data', 'StudentPerformaceFactorsClean.csv')\n",
    "model_path = os.path.join('models', 'pipeline_model.joblib')\n",
    "\n",
    "# Loading the cleaned dataset\n",
    "data_cleaned = pd.read_csv(cleaned_data_path)\n",
    "print(\"Cleaned dataset loaded successfully.\")\n",
    "\n",
    "# Loading the baseline model\n",
    "pipeline = joblib.load(model_path)\n",
    "print(\"Baseline model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Error Analysis Conclusions & Work Plan**\n",
    "   \n",
    "   - **Residual Distribution**\n",
    "     - **Observation**: Slight heteroscedasticity observed in residuals, with increased variance for higher predicted scores.\n",
    "     - **Action**: Investigate transformations of the target variable or features to stabilize variance.\n",
    "   \n",
    "   - **Subgroup Performance**\n",
    "     - **Observation**: Lower performance for students with Low and High parental involvement levels.\n",
    "     - **Action**: Explore interactions between parental involvement and other features or incorporate additional relevant features.\n",
    "   \n",
    "   - **Feature Importance**\n",
    "     - **Observation**: Attendance, Hours_Studied, and Previous_Scores are significant predictors, but other features may also play crucial roles.\n",
    "     - **Action**: Conduct feature engineering to create new features or transform existing ones to capture more information.\n",
    "   \n",
    "   - **Bias in Predictions**\n",
    "     - **Observation**: Slight tendency towards underestimation in predictions.\n",
    "     - **Action**: Adjust the model to reduce bias, possibly by addressing data imbalance or refining the loss function.\n",
    "   \n",
    "   - **Outliers**\n",
    "     - **Observation**: Presence of outliers affecting model performance.\n",
    "     - **Action**: Implement robust scaling or outlier detection methods to mitigate their impact.\n",
    "   \n",
    "2. **Work Plan**\n",
    "   \n",
    "   - **Hyperparameter Tuning**: Optimize XGBoost parameters to enhance model performance.\n",
    "   - **Feature Engineering**: Create new features and transform existing ones based on domain knowledge and EDA insights.\n",
    "   - **Handling Outliers**: Apply techniques to detect and handle outliers effectively.\n",
    "   - **Data Balancing**: If applicable, ensure that the model is not biased towards certain subgroups by balancing the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Improving Model Performance\n",
    "\n",
    "### a. Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning involves adjusting the model’s parameters to find the optimal configuration that minimizes error and maximizes performance. We’ll use GridSearchCV to perform an exhaustive search over specified parameter values for the XGBoost regressor.\n",
    "\n",
    "**Step-by-Step Implementation:**\n",
    "1. **Define Features and Target**\n",
    "2. **Identify Categorical and Numerical Features**\n",
    "3. **Handle Categorical Data**\n",
    "4. **Perform Train-Test Split**\n",
    "5. **Set Up and Run GridSearchCV**\n",
    "6. **Save the Improved Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Define Features and Target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = data_cleaned.drop('Exam_Score', axis=1)\n",
    "y = data_cleaned['Exam_Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Identify Categorical and Numerical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Handle Categorical Data**\n",
    "\n",
    "Convert boolean columns to integers if any exist. This ensures compatibility with machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean columns to integers if any\n",
    "for col in categorical_features:\n",
    "    if X[col].dtype == 'bool':\n",
    "        X[col] = X[col].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Perform Train-Test Split**\n",
    "\n",
    "Split the data into training and testing sets to evaluate model performance effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 5102 samples\n",
      "Testing set size: 1276 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Set Up and Run GridSearchCV**\n",
    "\n",
    "Define the parameter grid for XGBoost and initialize GridSearchCV to perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__subsample': [0.7, 0.8, 0.9],\n",
    "    'model__colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- **Parameter Grid (param_grid)**: Specifies the combinations of hyperparameters to search over.\n",
    "  - model__n_estimators: Number of boosting rounds. Higher values can improve performance but may increase training time.\n",
    "  - model__max_depth: Maximum depth of a tree. Controls the complexity of the model; deeper trees can capture more information but may overfit.\n",
    "  - model__learning_rate: Step size shrinkage used in update to prevent overfitting. Smaller values make the model more robust but require more boosting rounds.\n",
    "  - model__subsample: Subsample ratio of the training instances. Prevents overfitting by sampling a fraction of the training data.\n",
    "  - model__colsample_bytree: Subsample ratio of columns when constructing each tree. Helps in reducing overfitting by sampling a fraction of features.\n",
    "  \n",
    "- **GridSearchCV Initialization**:\n",
    "  - estimator: The pipeline loaded from Part 1.\n",
    "  - param_grid: The grid of hyperparameters.\n",
    "  - cv: Number of cross-validation folds.\n",
    "  - scoring: Metric to evaluate model performance (r2 in this case).\n",
    "  - verbose: Controls the verbosity level.\n",
    "  - n_jobs: Number of jobs to run in parallel (-1 uses all processors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting GridSearchCV:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "Hyperparameter tuning completed.\n"
     ]
    }
   ],
   "source": [
    "# Fit GridSearchCV\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Hyperparameter tuning completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieve Best Parameters and Score:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'model__colsample_bytree': 0.7, 'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 200, 'model__subsample': 0.7}\n",
      "Best R² Score from GridSearchCV: 0.6978\n"
     ]
    }
   ],
   "source": [
    "# Best parameters\n",
    "print(\"Best Parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Best R2 Score\n",
    "print(f\"Best R² Score from GridSearchCV: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update Pipeline with Best Estimator:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the pipeline with best parameters\n",
    "best_pipeline = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the Improved Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved model saved at 'models/improved_model_after_ht.joblib'.\n"
     ]
    }
   ],
   "source": [
    "# Save the improved model\n",
    "improved_model_path = os.path.join('models', 'improved_model_after_ht.joblib')\n",
    "joblib.dump(best_pipeline, improved_model_path)\n",
    "print(f\"Improved model saved at '{improved_model_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Feature Engineering\n",
    "\n",
    "Enhancing the dataset through feature engineering can capture additional patterns and relationships, potentially improving model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating Interaction Features\n",
    "Interaction features can capture the combined effect of two or more features on the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature engineering on the training set\n",
    "\n",
    "# Interaction between Hours_Studied and Motivation_Level\n",
    "X_train['Hours_Motivation_Interaction'] = X_train['Hours_Studied'] * X_train['Motivation_Level'].map({'Low': 1, 'Medium': 2, 'High': 3})\n",
    "X_test['Hours_Motivation_Interaction'] = X_test['Hours_Studied'] * X_test['Motivation_Level'].map({'Low': 1, 'Medium': 2, 'High': 3})\n",
    "\n",
    "# Interaction between Attendance and Parental_Involvement\n",
    "X_train['Attendance_Parental_Interaction'] = X_train['Attendance'] * X_train['Parental_Involvement'].map({'Low': 1, 'Medium': 2, 'High': 3})\n",
    "X_test['Attendance_Parental_Interaction'] = X_test['Attendance'] * X_test['Parental_Involvement'].map({'Low': 1, 'Medium': 2, 'High': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transforming Existing Features\n",
    "Applying transformations can help stabilize variance and make patterns more linear, which is beneficial for regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation of Previous_Scores to reduce skewness\n",
    "log_offset = 1  # Handle zero values\n",
    "X_train['Log_Previous_Scores'] = np.log1p(X_train['Previous_Scores'] + log_offset)\n",
    "X_test['Log_Previous_Scores'] = np.log1p(X_test['Previous_Scores'] + log_offset)\n",
    "\n",
    "# Polynomial Features for Hours_Studied\n",
    "X_train['Hours_Studied_Squared'] = X_train['Hours_Studied'] ** 2\n",
    "X_test['Hours_Studied_Squared'] = X_test['Hours_Studied'] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Encoding Categorical Features with Ordinal Encoding\n",
    "For ordinal categorical features, ordinal encoding can preserve the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Define ordinal features and their order\n",
    "ordinal_features = {\n",
    "    'Parental_Involvement': ['Low', 'Medium', 'High'],\n",
    "    'Access_to_Resources': ['Low', 'Medium', 'High'],\n",
    "    'Motivation_Level': ['Low', 'Medium', 'High'],\n",
    "    'Teacher_Quality': ['Low', 'Medium', 'High'],\n",
    "    'Peer_Influence': ['Negative', 'Neutral', 'Positive'],\n",
    "    'Parental_Education_Level': ['High School', 'College', 'Postgraduate'],\n",
    "    'Distance_from_Home': ['Near', 'Moderate', 'Far']\n",
    "}\n",
    "\n",
    "# Apply ordinal encoding\n",
    "for feature, order in ordinal_features.items():\n",
    "    ordinal_encoder = OrdinalEncoder(categories=[order])\n",
    "    X_train[f'{feature}_Encoded'] = ordinal_encoder.fit_transform(X_train[[feature]])\n",
    "    X_test[f'{feature}_Encoded'] = ordinal_encoder.transform(X_test[[feature]])\n",
    "\n",
    "# Drop the original ordinal features after encoding\n",
    "X_train.drop(columns=ordinal_features.keys(), inplace=True)\n",
    "X_test.drop(columns=ordinal_features.keys(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Updating Features in the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the pipeline...\n",
      "Pipeline fit completed.\n",
      "Updated preprocessor saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Exclude the original categorical features that have been encoded\n",
    "categorical_features = [feature for feature in X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist() if feature not in ordinal_features.keys()]\n",
    "\n",
    "# List of ordinal encoded features\n",
    "ordinal_encoded_features = [f\"{feature}_Encoded\" for feature in ordinal_features.keys()]\n",
    "\n",
    "# List of numerical features (including engineered numerical features)\n",
    "numeric_features = [feature for feature in X.select_dtypes(include=[np.number]).columns.tolist() if feature not in ordinal_encoded_features]\n",
    "numeric_features += ['Hours_Motivation_Interaction', 'Attendance_Parental_Interaction', 'Log_Previous_Scores', 'Hours_Studied_Squared']\n",
    "\n",
    "# Update preprocessor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "        ('ordinal', 'passthrough', ordinal_encoded_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Leave numeric features as is\n",
    ")\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "model_params = {key.replace('model__', ''): value for key, value in best_params.items()}\n",
    "\n",
    "# Update the pipeline with the new preprocessor\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', xgb.XGBRegressor(**model_params, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "print(\"Fitting the pipeline...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Pipeline fit completed.\")\n",
    "\n",
    "# Save updated pipeline\n",
    "joblib.dump(pipeline, 'models/improved_model_after_ht_and_fe.joblib')\n",
    "print(\"Updated preprocessor saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Model Performance Metrics:\n",
      "Mean Absolute Error (MAE): 0.68\n",
      "Mean Squared Error (MSE): 4.50\n",
      "Root Mean Squared Error (RMSE): 2.12\n",
      "R² Score: 0.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Making predictions with the improved model\n",
    "y_pred_improved = pipeline.predict(X_test)\n",
    "\n",
    "# Calculating evaluation metrics for the improved model\n",
    "mae_improved = mean_absolute_error(y_test, y_pred_improved)\n",
    "mse_improved = mean_squared_error(y_test, y_pred_improved)\n",
    "rmse_improved = np.sqrt(mse_improved)\n",
    "r2_improved = r2_score(y_test, y_pred_improved)\n",
    "\n",
    "print(\"Improved Model Performance Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_improved:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_improved:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_improved:.2f}\")\n",
    "print(f\"R² Score: {r2_improved:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
